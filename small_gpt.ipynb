{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-23T12:50:11.381800Z",
     "start_time": "2025-03-23T12:50:10.297861Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.nn.functional import embedding\n",
    "\n",
    "device =\"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\" # mac 上使用的\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # N卡使用\n",
    "print(f'using {device} device')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps device\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:39:00.078537Z",
     "start_time": "2025-03-23T14:39:00.075482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:  # 全局配置\n",
    "    window_size: int = 512   #  上下文长度\n",
    "    vocab_size: int = 50257 # 词汇表大小\n",
    "    embd_size: int = 768  #  嵌入的向量纬度\n",
    "    block_num: int = 12  # transformer block 重复数\n",
    "    head_num: int = 12  # 多头注意力机制 头数\n",
    "    head_size: int = int(embd_size/head_num) # 多头注意力每个头嵌入向量的纬度\n",
    "    dropout: float = 0.1  # 失活率\n",
    "\n",
    "conf = GPTConfig()\n",
    "print('conf:',conf)"
   ],
   "id": "e7785581a5ec5b10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf: GPTConfig(window_size=512, vocab_size=50257, embd_size=768, block_num=12, head_num=12, head_size=64, dropout=0.1)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:37:50.484001Z",
     "start_time": "2025-03-23T07:37:50.477660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = encoding.encode(\"............. deepseek\")\n",
    "raw = encoding.decode(encode)\n",
    "print('encode:',encode)\n",
    "print('raw:',raw)\n",
    "print(len('...................................'))"
   ],
   "id": "527790bd76d4d7f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode: [44274, 2769, 36163]\n",
      "raw: ............. deepseek\n",
      "35\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:42:04.255266Z",
     "start_time": "2025-03-16T16:42:04.239827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.encoding = tiktoken.get_encoding(\"gpt2\") # 获取 gpt2 的文本编码器\n",
    "\n",
    "        # 获取分词器的文本结束特殊标记\n",
    "        eot_token = self.encoding.encode(\"<|endoftext|>\",allowed_special={\"<|endoftext|>\"})\n",
    "        # 按行读取文本编码为 token 每行使用  endoftext token 标记\n",
    "        encoded = []\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                encoded.extend(self.encoding.encode(line) + eot_token)\n",
    "\n",
    "        # 将长文本分割成训练样本  每个最长上下文是 window_size 步长是 window_size\n",
    "        self.encoded_data = []\n",
    "        for i in range(0, len(encoded), conf.window_size):\n",
    "            # 多取一个 Token 作为目标\n",
    "            chunk = encoded[i:i + conf.window_size + 1] # python 不会越界，取不到就不取了\n",
    "            # 如果长度不够，用 eos_token 填充\n",
    "            if len(chunk) < conf.window_size + 1:\n",
    "                chunk = chunk + eot_token * (conf.window_size + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long) # 输入 前512 当训练数据\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long) # 输出 后512 当标签\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.encoding.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.encoding.decode(ids)\n",
    "\n",
    "dataset = MyDataset('input.txt')\n",
    "data_loader = DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "print('dataset_size',len(dataset),'loader_size',len(data_loader))\n",
    "x,y = next(iter(data_loader))\n",
    "print('x:',x[0][:10])\n",
    "print('y:',y[0][:10])"
   ],
   "id": "3cc400740d6fa479",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size 10 loader_size 1\n",
      "x: tensor([  257,  3200,  2861,  8208,   286,   465,     0,   314, 37901,   379])\n",
      "y: tensor([ 3200,  2861,  8208,   286,   465,     0,   314, 37901,   379,   262])\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T16:46:12.049535Z",
     "start_time": "2025-03-23T16:46:11.035668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module,Embedding,Sequential,Linear,LayerNorm,Dropout,ModuleList,GELU\n",
    "\n",
    "class SingleHeadAttention(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = Linear(conf.embd_size, conf.head_size)\n",
    "        self.value = Linear(conf.embd_size, conf.head_size)\n",
    "        self.query = Linear(conf.embd_size, conf.head_size)\n",
    "        # 下三角矩阵防止注意力关注到未来的 token\n",
    "        self.attention_mask=torch.tril(torch.ones(conf.window_size, conf.window_size, device=device))\n",
    "        self.dropout = Dropout(conf.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.key(x) # batch,window_size,head_size\n",
    "        v = self.value(x) # batch,window_size,head_size\n",
    "        q = self.query(x) # batch,window_size,head_size\n",
    "        weight = q @ k.transpose(-2, -1)  # batch window_size,window_size 第一纬是批量反转最后两纬 矩阵乘法获取权重\n",
    "        seq_len = x.shape[1]\n",
    "        # 一定要在 softmax 前除以 sqrt(head_size)\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0, # 长度未必每次都达到最长 512 防止 weight 越界\n",
    "            float('-inf') # 使用负无穷 softmax 后会变成 0\n",
    "        ) / math.sqrt(conf.embd_size)\n",
    "        weight = F.softmax(weight, dim=-1) # 忽略 batch 每一层 weight 位于 0～1 和为 1\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = ModuleList([SingleHeadAttention() for _ in range(conf.head_num)])\n",
    "        self.proj = Linear(conf.embd_size, conf.embd_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([head(x) for head in self.heads],dim=-1)\n",
    "        output = self.proj(output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 =  Linear(conf.embd_size, 4 * conf.embd_size) # 升纬\n",
    "        self.gelu = GELU()\n",
    "        self.l2 = Linear(4 * conf.embd_size, conf.embd_size) # 降纬\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(conf.embd_size)\n",
    "        self.att = MultiHeadAttention()\n",
    "        self.dropout1 = Dropout(conf.dropout)\n",
    "        self.ln2 = LayerNorm(conf.embd_size)\n",
    "        self.ffn = FeedForward()\n",
    "        self.dropout2 = Dropout(conf.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout1(self.att(self.ln1(x)))\n",
    "        x = x + self.dropout2(self.ffn(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "class GPT(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(conf.vocab_size, conf.embd_size)\n",
    "        self.position_embedding = Embedding(conf.window_size, conf.embd_size)\n",
    "        self.blocks = Sequential(*[Block() for _ in range(conf.block_num)])\n",
    "        self.layer_norm = LayerNorm(conf.embd_size)\n",
    "        self.output = Linear(conf.embd_size, conf.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # 获取嵌入编码与位置编码\n",
    "        token_emb = self.token_embedding(tokens)\n",
    "        seq_len = tokens.shape[1]\n",
    "        pos_emb = self.position_embedding(torch.arange(seq_len, device=device))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x) # 映射为每个词汇的概率\n",
    "        return self.output(x)\n",
    "\n",
    "gpt = GPT()\n",
    "gpt.to(device)\n",
    "total_params = sum(p.numel() for p in gpt.parameters())\n",
    "print('gpt:',gpt,'total_params:',total_params)\n",
    "# ones =torch.ones([3,3])\n",
    "# print('ones:',ones)\n",
    "# print('tril:',torch.tril(ones))"
   ],
   "id": "55070575f2e6321e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt: GPT(\n",
      "  (token_embedding): Embedding(50257, 768)\n",
      "  (position_embedding): Embedding(512, 768)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (output): Linear(in_features=768, out_features=50257, bias=False)\n",
      ") total_params: 162643968\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T16:29:35.781946Z",
     "start_time": "2025-03-23T16:29:35.778661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding = Embedding(3,5)\n",
    "res = embedding(torch.arange(0,3))\n",
    "print('res:',res)"
   ],
   "id": "2295f094b8093998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: tensor([[ 1.0240, -0.0236, -0.2139,  1.5125,  0.3323],\n",
      "        [-1.4283, -0.7845,  1.8703, -1.2259, -0.4763],\n",
      "        [ 2.1260,  0.9019,  1.2129,  0.1504, -0.5682]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:39:10.147574Z",
     "start_time": "2025-03-23T14:39:10.142646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp = torch.tril(torch.ones(5,5))\n",
    "print('temp:',temp)"
   ],
   "id": "6ee4e2e8db1e6630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:41:24.410602Z",
     "start_time": "2025-03-23T14:41:24.407333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp = torch.ones([5,5])\n",
    "print('temp:',temp)"
   ],
   "id": "59a061a6b0875352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:41:26.233529Z",
     "start_time": "2025-03-23T14:41:26.230550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dropout = Dropout(0.5)\n",
    "res= dropout(temp)\n",
    "print('res:',res)"
   ],
   "id": "5377ed2a4145d0d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: tensor([[2., 2., 0., 0., 2.],\n",
      "        [0., 2., 2., 0., 0.],\n",
      "        [0., 0., 2., 2., 0.],\n",
      "        [2., 0., 2., 0., 0.],\n",
      "        [2., 2., 0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:17:16.280135Z",
     "start_time": "2025-03-16T17:17:09.246366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gpt_gen(input,length=10):\n",
    "    tokens = torch.tensor(dataset.encode(input),device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            res = gpt(tokens.view(1,-1)) # 批次为 1\n",
    "            res = res[0,-1] # 批量训练的 只要第一个   句子也是批量训练的只要最后一个\n",
    "            probs = F.softmax(res,dim=0)\n",
    "            id_next = torch.multinomial(probs, num_samples=1) # 通过概率随机采样一个\n",
    "            tokens = torch.cat((tokens, id_next), dim=0) # 附加新的 token 这里超出  512 上下文长度会出问题\n",
    "    tokens =tokens.tolist()\n",
    "    return dataset.decode(tokens)\n",
    "\n",
    "print(gpt_gen(\"hello world\"))"
   ],
   "id": "69b80bc8a4f57001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world strateg Wales1111ug hexocker HOMELC foldingrative\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T16:48:36.405035Z",
     "start_time": "2025-03-23T16:48:35.872841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# gpt.load_state_dict(torch.load(\"gpt.pt\",weights_only=True)) # 加载参数\n",
    "optimizer = AdamW(gpt.parameters(), lr=3e-4)\n",
    "epoch = 50\n",
    "loss_arr =[]\n",
    "for epoch in range(epoch):\n",
    "    gpt.train() # 训练模式下  dropout 等函数会生效\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(data_loader):\n",
    "        # 转移数据\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # 前向传播\n",
    "        prob = gpt(x)\n",
    "        prob = prob.view(-1, conf.vocab_size) # 输出每个字符下的概率\n",
    "        y = y.view(prob.shape[0])     # 实际内容\n",
    "        loss = F.cross_entropy(prob, y) # 计算交叉熵\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # 调整参数\n",
    "        total_loss += loss.item()\n",
    "    print('epoch:',epoch,'total_loss:',total_loss)\n",
    "\n",
    "# torch.save(gpt.state_dict(),\"gpt.pt\") # 保存参数"
   ],
   "id": "669a1fa995c5d70c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m gpt\u001B[38;5;241m.\u001B[39mtrain() \u001B[38;5;66;03m# 训练模式下  dropout 等函数会生效\u001B[39;00m\n\u001B[1;32m      9\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mdata_loader\u001B[49m):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# 转移数据\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     13\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(gpt_gen(\"hello world\"))",
   "id": "f3c9dec821709c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(epoch+1), loss_arr)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)"
   ],
   "id": "df263b0fb54364e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
