{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T16:41:56.930109Z",
     "start_time": "2025-03-16T16:41:56.926231Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "device =\"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\" # mac 上使用的\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # N卡使用\n",
    "print(f'using {device} device')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps device\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:41:58.771796Z",
     "start_time": "2025-03-16T16:41:58.764109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:  # 全局配置\n",
    "    window_size: int = 512   #  上下文长度\n",
    "    vocab_size: int = 50257 # 词汇表大小\n",
    "    embd_size: int = 768  #  嵌入的向量纬度\n",
    "    block_num: int = 12  # transformer block 重复数\n",
    "    head_num: int = 12  # 多头注意力机制 头数\n",
    "    head_size: int = int(embd_size/head_num) # 多头注意力每个头嵌入向量的纬度\n",
    "    dropout: float = 0.1  # 失活率\n",
    "\n",
    "conf = GPTConfig()\n",
    "print('conf:',conf)"
   ],
   "id": "e7785581a5ec5b10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf: GPTConfig(window_size=512, vocab_size=50257, embd_size=768, block_num=12, head_num=12, head_size=64, dropout=0.1)\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:42:00.736848Z",
     "start_time": "2025-03-16T16:42:00.731272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = encoding.encode(\"hello world\")\n",
    "raw = encoding.decode(encode)\n",
    "print('encode:',encode,'raw:',raw)"
   ],
   "id": "527790bd76d4d7f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode: [31373, 995] raw: hello world\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:42:04.255266Z",
     "start_time": "2025-03-16T16:42:04.239827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.encoding = tiktoken.get_encoding(\"gpt2\") # 获取 gpt2 的文本编码器\n",
    "\n",
    "        # 获取分词器的文本结束特殊标记\n",
    "        eot_token = self.encoding.encode(\"<|endoftext|>\",allowed_special={\"<|endoftext|>\"})\n",
    "        # 按行读取文本编码为 token 每行使用  endoftext token 标记\n",
    "        encoded = []\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                encoded.extend(self.encoding.encode(line) + eot_token)\n",
    "\n",
    "        # 将长文本分割成训练样本  每个最长上下文是 window_size 步长是 window_size\n",
    "        self.encoded_data = []\n",
    "        for i in range(0, len(encoded), conf.window_size):\n",
    "            # 多取一个 Token 作为目标\n",
    "            chunk = encoded[i:i + conf.window_size + 1] # python 不会越界，取不到就不取了\n",
    "            # 如果长度不够，用 eos_token 填充\n",
    "            if len(chunk) < conf.window_size + 1:\n",
    "                chunk = chunk + eot_token * (conf.window_size + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long) # 输入 前512 当训练数据\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long) # 输出 后512 当标签\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.encoding.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.encoding.decode(ids)\n",
    "\n",
    "dataset = MyDataset('input.txt')\n",
    "data_loader = DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "print('dataset_size',len(dataset),'loader_size',len(data_loader))\n",
    "x,y = next(iter(data_loader))\n",
    "print('x:',x[0][:10])\n",
    "print('y:',y[0][:10])"
   ],
   "id": "3cc400740d6fa479",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size 10 loader_size 1\n",
      "x: tensor([  257,  3200,  2861,  8208,   286,   465,     0,   314, 37901,   379])\n",
      "y: tensor([ 3200,  2861,  8208,   286,   465,     0,   314, 37901,   379,   262])\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:45:31.129441Z",
     "start_time": "2025-03-16T16:45:27.445109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module,Embedding,Sequential,Linear,LayerNorm,Dropout,ModuleList,GELU\n",
    "\n",
    "class SingleHeadAttention(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = Linear(conf.embd_size, conf.head_size)\n",
    "        self.value = Linear(conf.embd_size, conf.head_size)\n",
    "        self.query = Linear(conf.embd_size, conf.head_size)\n",
    "        # 下三角矩阵防止注意力关注到未来的 token\n",
    "        self.attention_mask=torch.tril(torch.ones(conf.window_size, conf.window_size, device=device))\n",
    "        self.dropout = Dropout(conf.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.key(x) # batch,window_size,head_size\n",
    "        v = self.value(x) # batch,window_size,head_size\n",
    "        q = self.query(x) # batch,window_size,head_size\n",
    "        weight = q @ k.transpose(-2, -1)  # batch window_size,window_size 第一纬是批量反转最后两纬 矩阵乘法获取权重\n",
    "        seq_len = x.shape[1]\n",
    "        # 一定要在 softmax 前除以 sqrt(head_size)\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0, # 长度未必每次都达到最长 512 防止 weight 越界\n",
    "            float('-inf') # 使用负无穷 softmax 后会变成 0\n",
    "        ) / math.sqrt(conf.embd_size)\n",
    "        weight = F.softmax(weight, dim=-1) # 忽略 batch 每一层 weight 位于 0～1 和为 1\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = ModuleList([SingleHeadAttention() for _ in range(conf.head_num)])\n",
    "        self.proj = Linear(conf.embd_size, conf.embd_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([head(x) for head in self.heads],dim=-1)\n",
    "        output = self.proj(output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 =  Linear(conf.embd_size, 4 * conf.embd_size) # 升纬\n",
    "        self.gelu = GELU()\n",
    "        self.l2 = Linear(4 * conf.embd_size, conf.embd_size) # 降纬\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(conf.embd_size)\n",
    "        self.att = MultiHeadAttention()\n",
    "        self.dropout1 = Dropout(conf.dropout)\n",
    "        self.ln2 = LayerNorm(conf.embd_size)\n",
    "        self.ffn = FeedForward()\n",
    "        self.dropout2 = Dropout(conf.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout1(self.att(self.ln1(x)))\n",
    "        x = x + self.dropout2(self.ffn(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "class GPT(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(conf.vocab_size, conf.embd_size)\n",
    "        self.position_embedding = Embedding(conf.window_size, conf.embd_size)\n",
    "        self.blocks = Sequential(*[Block() for _ in range(conf.block_num)])\n",
    "        self.layer_norm = LayerNorm(conf.embd_size)\n",
    "        self.output = Linear(conf.embd_size, conf.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # 获取嵌入编码与位置编码\n",
    "        token_emb = self.token_embedding(tokens)\n",
    "        seq_len = tokens.shape[1]\n",
    "        pos_emb = self.position_embedding(torch.arange(seq_len, device=device))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.output(x)\n",
    "\n",
    "gpt = GPT()\n",
    "gpt.to(device)\n",
    "total_params = sum(p.numel() for p in gpt.parameters())\n",
    "print('gpt:',gpt,'total_params:',total_params)\n",
    "ones =torch.ones([3,3])\n",
    "print('ones:',ones)\n",
    "print('tril:',torch.tril(ones))   # TODO 用到的函数单独演示一下"
   ],
   "id": "55070575f2e6321e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt: GPT(\n",
      "  (token_embedding): Embedding(50257, 768)\n",
      "  (position_embedding): Embedding(512, 768)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): Block(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (att): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x SingleHeadAttention(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (output): Linear(in_features=768, out_features=50257, bias=False)\n",
      ") total_params: 162643968\n",
      "ones: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tril: tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:17:16.280135Z",
     "start_time": "2025-03-16T17:17:09.246366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gpt_gen(input,length=10):\n",
    "    tokens = torch.tensor(dataset.encode(input),device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            res = gpt(tokens.view(1,-1)) # 批次为 1\n",
    "            res = res[0,-1] # 批量训练的 只要第一个   句子也是批量训练的只要最后一个\n",
    "            probs = F.softmax(res,dim=0)\n",
    "            id_next = torch.multinomial(probs, num_samples=1) # 通过概率随机采样一个\n",
    "            tokens = torch.cat((tokens, id_next), dim=0) # 附加新的 token 这里超出  512 上下文长度会出问题\n",
    "    tokens =tokens.tolist()\n",
    "    return dataset.decode(tokens)\n",
    "\n",
    "print(gpt_gen(\"hello world\"))"
   ],
   "id": "69b80bc8a4f57001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world strateg Wales1111ug hexocker HOMELC foldingrative\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:57:34.490932Z",
     "start_time": "2025-03-16T16:57:19.380168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# gpt.load_state_dict(torch.load(\"gpt.pt\",weights_only=True)) # 加载参数\n",
    "optimizer = AdamW(gpt.parameters(), lr=3e-4)\n",
    "epoch = 50\n",
    "for epoch in range(epoch):\n",
    "    gpt.train() # 训练模式下  dropout 等函数会生效\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(data_loader):\n",
    "        # 转移数据\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # 前向传播\n",
    "        prob = gpt(x)\n",
    "        prob = prob.view(-1, conf.vocab_size) # 输出每个字符下的概率\n",
    "        y = y.view(prob.shape[0])     # 实际内容\n",
    "        loss = F.cross_entropy(prob, y) # 计算交叉熵\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # 调整参数\n",
    "        total_loss += loss.item()\n",
    "    print('epoch:',epoch,'total_loss:',total_loss)\n",
    "\n",
    "# torch.save(gpt.state_dict(),\"gpt.pt\") # 保存参数"
   ],
   "id": "669a1fa995c5d70c",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[74], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m prob \u001B[38;5;241m=\u001B[39m \u001B[43mgpt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m prob \u001B[38;5;241m=\u001B[39m prob\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, conf\u001B[38;5;241m.\u001B[39mvocab_size) \u001B[38;5;66;03m# 输出每个字符下的概率\u001B[39;00m\n\u001B[1;32m     16\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mview(prob\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])     \u001B[38;5;66;03m# 实际内容\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[68], line 85\u001B[0m, in \u001B[0;36mGPT.forward\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m     83\u001B[0m pos_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding(torch\u001B[38;5;241m.\u001B[39marange(seq_len, device\u001B[38;5;241m=\u001B[39mdevice))\n\u001B[1;32m     84\u001B[0m x \u001B[38;5;241m=\u001B[39m token_emb \u001B[38;5;241m+\u001B[39m pos_emb\n\u001B[0;32m---> 85\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(x)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(x)\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[68], line 66\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 66\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43matt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     67\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mffn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln2(x)))\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[68], line 38\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 38\u001B[0m     output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([head(x) \u001B[38;5;28;01mfor\u001B[39;00m head \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads],dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     39\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(output)\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "Cell \u001B[0;32mIn[68], line 38\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 38\u001B[0m     output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m head \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads],dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     39\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(output)\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[68], line 27\u001B[0m, in \u001B[0;36mSingleHeadAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     22\u001B[0m weight \u001B[38;5;241m=\u001B[39m weight\u001B[38;5;241m.\u001B[39mmasked_fill(\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_mask[:seq_len, :seq_len] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;66;03m# 长度未必每次都达到最长 512 防止 weight 越界\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-inf\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# 使用负无穷 softmax 后会变成 0\u001B[39;00m\n\u001B[1;32m     25\u001B[0m ) \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(conf\u001B[38;5;241m.\u001B[39membd_size)\n\u001B[1;32m     26\u001B[0m weight \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(weight, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# 忽略 batch 每一层 weight 位于 0～1 和为 1\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m out \u001B[38;5;241m=\u001B[39m weight \u001B[38;5;241m@\u001B[39m v\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001B[0m, in \u001B[0;36mDropout.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/llm_demo/.venv/lib/python3.8/site-packages/torch/nn/functional.py:1295\u001B[0m, in \u001B[0;36mdropout\u001B[0;34m(input, p, training, inplace)\u001B[0m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mdropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(gpt_gen(\"hello world\"))",
   "id": "f3c9dec821709c93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
